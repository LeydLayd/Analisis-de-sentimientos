{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importacion de librerias"
      ],
      "metadata": {
        "id": "88QvWgN_gC3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "KcM_ATVASjhs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clonacion del repositorio"
      ],
      "metadata": {
        "id": "3G5kk5eQUKaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LeydLayd/Analisis-de-sentimientos.git\n"
      ],
      "metadata": {
        "id": "e5ak373BUJQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "gq47uRknalhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_RfpUTJMLgg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Descargar stopwords de NLTK\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Configuración de los directorios\n",
        "repo_dir = \"Analisis-de-sentimientos/\"\n",
        "neg_dir = os.path.join(repo_dir, \"neg\")\n",
        "pos_dir = os.path.join(repo_dir, \"pos\")\n",
        "\n",
        "# Cargar stopwords en inglés\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Eliminar signos de puntuación usando expresiones regulares\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Convertir a minúsculas\n",
        "    text = text.lower()\n",
        "\n",
        "    # Eliminar palabras que son solo signos de puntuación o que contienen números\n",
        "    text = ' '.join([word for word in text.split() if word.isalpha()])\n",
        "\n",
        "    # Eliminar palabras vacías (stopwords)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "    # Eliminar términos cortos (menos de 3 caracteres)\n",
        "    text = ' '.join([word for word in text.split() if len(word) > 2])\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_and_process_data(directory):\n",
        "    documents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), 'r', encoding='latin-1') as file:\n",
        "            text = file.read()\n",
        "            text = preprocess_text(text)\n",
        "            documents.append(text)\n",
        "    return documents\n",
        "\n",
        "\n",
        "# Cargar datos de reseñas negativas y positivas\n",
        "neg_reviews = load_and_process_data(neg_dir)\n",
        "pos_reviews = load_and_process_data(pos_dir)\n",
        "\n",
        "# Separar las últimas 100 opiniones para prueba y el resto para entrenamiento\n",
        "train_neg = neg_reviews[:-100]\n",
        "test_neg = neg_reviews[-100:]\n",
        "train_pos = pos_reviews[:-100]\n",
        "test_pos = pos_reviews[-100:]\n",
        "\n",
        "# Unir datos de entrenamiento y prueba\n",
        "train_docs = train_neg + train_pos\n",
        "test_docs = test_neg + test_pos\n",
        "\n",
        "# Etiquetas: 0 para negativo, 1 para positivo\n",
        "train_labels = [0] * len(train_neg) + [1] * len(train_pos)\n",
        "test_labels = [0] * len(test_neg) + [1] * len(test_pos)\n",
        "\n",
        "# Crear el Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "# Convertir textos a TF-IDF\n",
        "X_train = tokenizer.texts_to_matrix(train_docs, mode='tfidf')\n",
        "X_test = tokenizer.texts_to_matrix(test_docs, mode='tfidf')\n",
        "\n",
        "# Convertir las etiquetas en formato categórico (para clasificación)\n",
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "#modelo secuencial\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)), #128 es la red neuronal el inpu_shape es la entradas que es todo el vocabulario\n",
        "    Dense(64, activation='relu'), # son 3 capaz\n",
        "    Dense (32, activation = 'relu'),\n",
        "    Dense(1, activation='sigmoid')  # Para clasificación binaria sigmoid es para hacerlo dos clases\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predicciones\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "# Métricas de evaluación\n",
        "print(\"\\nPrecisión:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nReporte de clasificación:\\n\", classification_report(y_test, y_pred, target_names=[\"Negativo\", \"Positivo\"]))\n",
        "\n",
        "# Calcular y mostrar la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nMatriz de Confusión:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pruebas"
      ],
      "metadata": {
        "id": "YSQY1FePaqZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una función para predecir el sentimiento de un texto\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    # Preprocesar el texto\n",
        "    processed_text = preprocess_text(text)\n",
        "\n",
        "    # Convertir el texto a la representación TF-IDF\n",
        "    text_tfidf = tokenizer.texts_to_matrix([processed_text], mode='tfidf')\n",
        "\n",
        "    # Realizar la predicción\n",
        "    prediction = model.predict(text_tfidf)[0][0]\n",
        "\n",
        "    # Interpretar el resultado\n",
        "    sentiment = \"Positivo\" if prediction > 0.5 else \"Negativo\"\n",
        "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
        "\n",
        "    return sentiment, confidence\n",
        "\n",
        "from google.colab import files\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "import io\n",
        "\n",
        "# Crear un widget para cargar archivos\n",
        "upload_button = widgets.Button(\n",
        "    description='Cargar archivo .txt',\n",
        "    button_style='info',\n",
        "    tooltip='Haz clic para cargar un archivo de texto'\n",
        ")\n",
        "\n",
        "analyze_button = widgets.Button(\n",
        "    description='Analizar Sentimiento',\n",
        "    button_style='primary',\n",
        "    tooltip='Haz clic para analizar el sentimiento del texto',\n",
        "    disabled=True  # Inicialmente deshabilitado hasta que se cargue un archivo\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "file_content = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder='El contenido del archivo se mostrará aquí...',\n",
        "    description='Contenido:',\n",
        "    disabled=False,\n",
        "    layout={'width': '100%', 'height': '200px'}\n",
        ")\n",
        "\n",
        "file_info = widgets.HTML(value=\"<b>Ningún archivo cargado</b>\")\n",
        "uploaded_content = None\n",
        "\n",
        "def on_upload_clicked(b):\n",
        "    global uploaded_content\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        print(\"Selecciona un archivo .txt de tu computadora...\")\n",
        "\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "\n",
        "            if uploaded:\n",
        "                file_name = list(uploaded.keys())[0]\n",
        "                if not file_name.endswith('.txt'):\n",
        "                    print(f\"Advertencia: El archivo '{file_name}' no tiene extensión .txt, pero intentaremos procesarlo.\")\n",
        "\n",
        "                content = uploaded[file_name]\n",
        "                try:\n",
        "                    # Intentar decodificar como UTF-8\n",
        "                    text_content = content.decode('utf-8')\n",
        "                except UnicodeDecodeError:\n",
        "                    # Si falla, intentar con latin-1\n",
        "                    try:\n",
        "                        text_content = content.decode('latin-1')\n",
        "                    except:\n",
        "                        text_content = content.decode('utf-8', errors='replace')\n",
        "\n",
        "                file_content.value = text_content\n",
        "                uploaded_content = text_content\n",
        "                file_info.value = f\"<b>Archivo cargado:</b> {file_name} ({len(text_content)} caracteres)\"\n",
        "                analyze_button.disabled = False\n",
        "                print(f\"Archivo '{file_name}' cargado correctamente. Haz clic en 'Analizar Sentimiento'.\")\n",
        "            else:\n",
        "                print(\"No se seleccionó ningún archivo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al cargar el archivo: {str(e)}\")\n",
        "\n",
        "def on_analyze_clicked(b):\n",
        "    global uploaded_content\n",
        "\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        if not uploaded_content or uploaded_content.strip() == '':\n",
        "            print(\"No hay contenido para analizar. Por favor, carga un archivo primero.\")\n",
        "            return\n",
        "\n",
        "        print(\"Analizando sentimiento del archivo...\")\n",
        "        sentiment, confidence = predict_sentiment(uploaded_content, model, tokenizer)\n",
        "        print(f\"Sentimiento: {sentiment}\")\n",
        "        print(f\"Confianza: {confidence:.2f}\")\n",
        "\n",
        "        # Análisis adicional - palabras más relevantes\n",
        "        words = uploaded_content.lower().split()\n",
        "        preprocessed_words = [word for word in words if word.isalpha() and word not in stop_words and len(word) > 2]\n",
        "\n",
        "        print(f\"\\nTotal de palabras procesadas: {len(preprocessed_words)}\")\n",
        "        print(\"\\nMuestra de palabras clave analizadas (primeras 20):\")\n",
        "        print(\", \".join(preprocessed_words[:20]))\n",
        "\n",
        "        # Estadísticas adicionales\n",
        "        word_count = len(uploaded_content.split())\n",
        "        char_count = len(uploaded_content)\n",
        "        print(f\"\\nEstadísticas del texto: {word_count} palabras, {char_count} caracteres\")\n",
        "\n",
        "upload_button.on_click(on_upload_clicked)\n",
        "analyze_button.on_click(on_analyze_clicked)\n",
        "\n",
        "# Mostrar los widgets\n",
        "print(\"Análisis de sentimiento de archivos de texto\")\n",
        "display(upload_button)\n",
        "display(file_info)\n",
        "display(file_content)\n",
        "display(analyze_button)\n",
        "display(output)"
      ],
      "metadata": {
        "id": "cx6UyikrZUqP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}